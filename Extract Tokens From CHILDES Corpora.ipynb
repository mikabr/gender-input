{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "options(stringsAsFactors = FALSE)\n",
    "source(\"CLANtoR.R\")\n",
    "source(\"config.R\")\n",
    "library('parallel','plyr','tools')\n",
    "wordsToExclude = c(\"hmm\", \"hm\", \"mm\", \"uh\", \"uhh\", \"ah\", \"um\",\"=laughs\", \"uhhuh\",\"eh\",\"xxx\",\"yyy\", \"xx\", 'yy','aw', 'www','er','ka')\n",
    "noUtt = c(\"0\")\n",
    "metadataRows = c('sentGloss','sentMor','Speaker','xgr','pho','act','gpx','sit','com', 'par','Filename','Participants',\n",
    "                 'Date',\"Language\",\"Corpus\",\"Age\",\"Gender\",\"Utt.Number\",\"index\",\"add\",\"alt\",\"int\",\"spa\",\"err\",\"eng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentenceHandler = function(row){\n",
    "    #!!! the problem is that there are many sentences where the word nodes do not have all of the metadata, \n",
    "    #leading to mismatches in the length of the arrays    \n",
    "    \n",
    "    glosses = cleanGloss(row$Gloss)    \n",
    "    mors = cleanMOR(row$mor)       \n",
    "    \n",
    "    #handle any mismatches in length:\n",
    "    if (length(glosses) > length(mors)){\n",
    "        # handle a mismatch in the length of the two cleaned vectors    \n",
    "        \n",
    "        #Why not use the gloss to index into the mors, and find the corresponding term for each?\n",
    "        #b/c there's aproblem with indexing in from the gloss is that there are complex terms \n",
    "        #like don't == aux|do~neg|not        \n",
    "        \n",
    "        newMor = mat.or.vec(length(mors),1)\n",
    "        offset = 0 #this is the difference between the index in glosses and the index in mor\n",
    "        \n",
    "        for (i in 1:length(glosses)){\n",
    "            if(glosses[i] %in% wordsToExclude){\n",
    "                newMor[i] = 'EXC'       \n",
    "                offset = offset + 1\n",
    "            } else if (glosses[i] %in% noUtt){\n",
    "                newMor[i] = 'NO_UTT'\n",
    "                offset = offset + 1\n",
    "            } else {\n",
    "                newMor[i] = mors[i-offset]\n",
    "            }            \n",
    "        }\n",
    "        \n",
    "        if(length(glosses) != length(newMor)){\n",
    "            print('Glosses: ')\n",
    "            print(glosses)\n",
    "            print('Original Mors:')\n",
    "            print(mors)\n",
    "            print('Corrected Mors:')\n",
    "            print(newMor)\n",
    "            stop('Recovery process for longer gloss failed')\n",
    "        } else{\n",
    "            mors = newMor            \n",
    "        }\n",
    "    } else if (length(glosses) < length(mors)){         \n",
    "        print('Glosses: ')\n",
    "        print(glosses)\n",
    "        print('Mors:')\n",
    "        print(mors)\n",
    "        print('Row')\n",
    "        print(row)\n",
    "        stop('No recovery process for longer MOR line')\n",
    "    }\n",
    "    \n",
    "    splitGlosses = strsplit(glosses,'@') \n",
    "    glosses = sapply(splitGlosses, function(x){x[1]})\n",
    "    atTags = sapply(splitGlosses, function(x){ ifelse(length(x) > 1,x[2],'NA')})    \n",
    "    \n",
    "    rd = data.frame(Gloss = glosses, mor = mors, atTags)\n",
    "    if (nrow(rd) > 0){                        \n",
    "        row$sentGloss = paste(glosses, collapse = ' ')\n",
    "        row$sentMor = paste(mors, collapse= ' ')\n",
    "        if('xgr' %in% names(row)){\n",
    "            row$xgr = gsub('\\\\t','',row$xgr)            \n",
    "        }\n",
    "        selectRows = names(row)[names(row) %in% metadataRows] #only select those from the desired columns that are present\n",
    "        return(cbind(rd, row[,selectRows], row.names = NULL)) #returns df, number of words * columns\n",
    "    } else {\n",
    "        print(row)\n",
    "        stop('Zero-length return data')\n",
    "    }  \n",
    "}\n",
    "\n",
    "cleanGloss = function(gloss){\n",
    "    #print('cleaning gloss')\n",
    "    originalGloss = gloss\n",
    "    gloss = gsub('<.*?>','',gloss) #remove reformulations from the gloss\n",
    "    gloss = gsub(\"[^[:alnum:][:space:]'_@+]\", '', gloss)  #remove non-apostrophe punctuation  \n",
    "    gloss = gsub('\\342\\200\\234','\\342\\200\\234 ', gloss)#preceding quotes\n",
    "    gloss = gsub('\\342\\200\\235',' \\342\\200\\235', gloss)#following quotes\n",
    "    unlistedwords = unlist(strsplit(gloss, split = \" \"))\n",
    "    unlistedwords = unlistedwords[!(unlistedwords ==  '')]\n",
    "    unlistedwords = gsub(\"\\\\n|\\\\t\", \" \", unlistedwords)           \n",
    "    unlistedwords = unlistedwords[sapply(gsub('[[:punct:]]','', unlistedwords), nchar) > 0] #remove puntucation-only words\n",
    "    #is the last items a number with > 7 digits? this is a tag, don't return it\n",
    "    if (length(unlistedwords) == 0){\n",
    "        print(originalGloss)\n",
    "    }\n",
    "    \n",
    "    if(nchar(tail(unlistedwords,1)) >= 5 & nchar(gsub('[[:digit:]_+] *','',tail(unlistedwords,1))) == 0){\n",
    "        return(unlistedwords[1:length(unlistedwords)-1]) \n",
    "    } else {\n",
    "        return(unlistedwords)\n",
    "    }\n",
    "}\n",
    "\n",
    "cleanMOR = function(mor){\n",
    "    #print('cleaning mor')\n",
    "    onesplit = gsub(\"cm\\\\|cm|none\\\\|cm\", \"\", unlist(strsplit(mor, \" \")))\n",
    "    onesplit = onesplit[grepl(\"\\\\|\", onesplit)]\n",
    "    onesplit = gsub(\"\\\\n|\\\\t\", \" \", onesplit)\n",
    "    onesplit = unlist(strsplit(onesplit, split = \" \"))\n",
    "    onesplit = gsub(\"[!,?//.]\", \"\", onesplit) #!!! think we probably want to keep this information around\n",
    "    onesplit = onesplit[!(onesplit %in%  c(\"\",\"bq|bq\",\"eq|eq\"))]        \n",
    "    return(onesplit[sapply(gsub('[[:punct:]]','', onesplit), nchar) > 0])\n",
    "}\n",
    "\n",
    "processClanFile = function(filename){\n",
    "    print(paste('Processing file:', filename))\n",
    "    df = read.CLAN.file(filename)\n",
    "    if (ncol(df) > 35){ #!!! lower this number if possible\n",
    "        print(names(df))\n",
    "        stop(paste(filename, 'has an invalid structure: too many columns found'))\n",
    "    }\n",
    "    print(paste('CLANtoR produced dataframe with dimensions:',dim(df)[1], 'by', dim(df)[2]))\n",
    "    \n",
    "    processedSentenceList = lapply(1:nrow(df), function(i){sentenceHandler(df[i,])})\n",
    "    print('Processed sentences')\n",
    "    \n",
    "    allTokens = do.call('rbind.fill', processedSentenceList)            \n",
    "    return(allTokens)\n",
    "}\n",
    "\n",
    "\n",
    "processDirectory = function(dirname){    \n",
    "    fnames = paste(dirname, list.files(dirname, recursive=T, pattern = \"\\\\.cha$\"), sep='/')\n",
    "    print(paste('Processing', length(fnames), 'filenames'))\n",
    "    \n",
    "    #!!! multicore this \n",
    "    allFiles = do.call('rbind.fill', lapply(fnames, processClanFile))\n",
    "    #allFiles = do.call('rbind.fill', mclapply(fnames, processClanFile, mc.cores=detectCores()))\n",
    "    names(allFiles) = tolower(names(allFiles))\n",
    "    allFiles$age = sapply(allFiles$age, ageToDays)\n",
    "    \n",
    "    return(allFiles)\n",
    "}\n",
    "\n",
    "ageToDays = function(age){\n",
    "    ageParts = strsplit(age, ';')[[1]]\n",
    "    return(ceiling((12*30.5*as.numeric(ageParts[1])) + as.numeric(ageParts[2])*30.5))\t\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Processing 28 filenames\"\n",
      "[1] \"Processing file: /shared_hd0/corpora/childes_new/Bloom70/Eric/eric1.cha\"\n",
      "[1] \"CLANtoR produced dataframe with dimensions: 1889 by 27\"\n",
      "[1] \"<what is that> \"\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in if (nchar(tail(unlistedwords, 1)) >= 5 & nchar(gsub(\"[[:digit:]_+] *\", : argument is of length zero\n",
     "output_type": "error",
     "traceback": [
      "Error in if (nchar(tail(unlistedwords, 1)) >= 5 & nchar(gsub(\"[[:digit:]_+] *\", : argument is of length zero\n"
     ]
    }
   ],
   "source": [
    "#to start it, point it at a specific dictionary\n",
    "bloom70 = processDirectory('/shared_hd0/corpora/childes_new/Bloom70') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: DBI\n"
     ]
    }
   ],
   "source": [
    "#connect R to mysql\n",
    "library('RMySQL')\n",
    "childes_db = dbConnect(MySQL(), user=config[['username']], password=config[['password']], dbname=config[['dbasename']], host=config[['host']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#write the dataframe to the remote\n",
    "\n",
    "dbWriteTable(childes_db, bloom70, name = \"words\", row.names = F, overwrite=T)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Processing 52 filenames\"\n"
     ]
    }
   ],
   "source": [
    "#to start it, point it at a specific dictionary\n",
    "suppes = processDirectory('/shared_hd0/corpora/childes_new/Suppes') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dbWriteTable(childes_db, suppes, name = \"words\", row.names = F, append=T)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Processing 364 filenames\"\n"
     ]
    }
   ],
   "source": [
    "providence = processDirectory('/shared_hd0/corpora/childes_new/Providence') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "TRUE"
      ],
      "text/latex": [
       "TRUE"
      ],
      "text/markdown": [
       "TRUE"
      ],
      "text/plain": [
       "[1] TRUE"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbWriteTable(childes_db, providence, name = \"words\", row.names = F, append=T)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Need some reliable desgination of the child\n",
    "#break apart the corpus schema"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
